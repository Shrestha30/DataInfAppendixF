argh==0.29.4
datasets==2.13.1
evaluate==0.4.0
numpy==1.24.4
numpy==1.25.2
pandas==2.0.3
peft==0.5.0
torch==2.0.1
tqdm==4.65.0
transformers==4.34.0.dev0

Q1.1: Our Bangla-specific hate speech explanation model supports transparency and accountability in automated moderation. By generating native-language explanations, it empowers content moderators to make more informed decisions and helps researchers and civic institutions better understand culturally specific expressions of hate. This enhances fairness and inclusivity in multilingual NLP systems.
Q1.2: : We focused on subjective human ratings to assess nuanced aspects of explanations—like persuasiveness and soundness—that current automatic metrics struggle to capture, especially given their limited sensitivity to cultural distinctions.
Q1.3: The 1,000 samples used for fine-tuning were drawn from the training and validation splits of the Bd-SHS dataset. In contrast, for 60 explanations evaluated by participants, the texts were curated exclusively from the test set, ensuring no overlap and maintaining an unbiased evaluation.

Q2: None

Q3.1: We selected the untuned Mistral-Instruct model as a baseline due to its strong zero-shot capabilities on instruction-following tasks, including explanation generation. While we evaluated BanglaT5 under similar conditions, it struggled to produce coherent explanations with our 1,000 training samples. This is likely because BanglaT5's pretraining focused on tasks such as question answering and multi-turn dialogue—typically requiring orders of magnitude more data. In contrast, Mistral's instruction tuning aligned more naturally with our task without further fine-tuning.
Justify the selection of untuned Mistral as a baseline: Were alternative baseline models (e.g., BanglaT5) evaluated under comparable conditions?
Just a sample answer, you could use or deny it: Yes we have tried BanglaT5, but it fails to generate proper explanation with only 1000 samples. Among the explored tasks by Bangla T5, most similar tasks to ours are "Question Answering" and "Multi-tun dialog". Both of which have an average training size of 100k. Moreover Mistral instruct model we used was already trained for such tasks whereareas, Bangla T5 were not.

